# Gris-Search-and-Random-search-extern-
Grid search and random search are two methods for tuning the hyperparameters of a machine learning model. Hyperparameters are the settings of a model that are not learned from the data. They can have a significant impact on the performance of the model.

Grid search is an exhaustive search of a hyperparameter space. This means that it will try every possible combination of hyperparameter values. This can be computationally expensive, especially if there are many hyperparameters.

Random search is a less computationally expensive alternative to grid search. It randomly samples hyperparameter values from a distribution. This means that it will not try every possible combination of hyperparameter values, but it is more likely to find a good combination than if it randomly sampled values without replacement.

The best method for tuning hyperparameters depends on the specific problem. Grid search is more likely to find the best hyperparameters, but it can be too computationally expensive for some problems. Random search is less computationally expensive, but it is less likely to find the best hyperparameters.

Here is a table that summarizes the key differences between grid search and random search:

Feature	Grid Search	Random Search
Exhaustive search	Yes	No
Computationally expensive	Yes	No
More likely to find best hyperparameters	Yes	No
Here are some additional tips for tuning hyperparameters:

Start with a small number of hyperparameters. If you have too many hyperparameters, it can be difficult to find the best combination.
Use a validation set. The validation set should be used to evaluate the performance of the model with different hyperparameters.
Use a scoring metric. The scoring metric should be used to compare the performance of the model with different hyperparameters.
Use a cross-validation technique. Cross-validation can be used to reduce the variance of the scoring metric.
By following these tips, you can improve the performance of your machine learning models by tuning the hyperparameters.

for furthur go through notebook for hand-on practice . 
